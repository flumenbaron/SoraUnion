{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Import necessary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2024-12-04T10:59:47.686606Z",
       "execution_start_time": "2024-12-04T10:59:47.5302299Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "9df837a0-9b89-44a0-854c-0e69afff9615",
       "queued_time": "2024-12-04T10:59:47.3914793Z",
       "session_id": "36",
       "session_start_time": null,
       "spark_jobs": null,
       "spark_pool": "prdneudwcesyns",
       "state": "finished",
       "statement_id": 36,
       "statement_ids": [
        36
       ]
      },
      "text/plain": [
       "StatementMeta(prdneudwcesyns, 36, 36, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "from pyspark.sql.functions import col, max, year, month\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Read CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {},
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2024-12-04T11:00:33.0680202Z",
       "execution_start_time": "2024-12-04T10:59:47.806577Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "6829f4f4-a59d-4e69-ae5c-bad0f8702ab7",
       "queued_time": "2024-12-04T10:59:47.4586412Z",
       "session_id": "36",
       "session_start_time": null,
       "spark_jobs": null,
       "spark_pool": "prdneudwcesyns",
       "state": "finished",
       "statement_id": 37,
       "statement_ids": [
        37
       ]
      },
      "text/plain": [
       "StatementMeta(prdneudwcesyns, 36, 37, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = spark.read.load(\n",
    "    'abfss://filesystem@flumenbaron.dfs.core.windows.net/synapse/workspaces/warehouse/all_stock_data.csv',\n",
    "    format='csv',\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Count the rows in the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2024-12-04T11:00:40.0257917Z",
       "execution_start_time": "2024-12-04T11:00:33.19088Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "7c318714-e3b2-4991-b949-37cd4f7d2545",
       "queued_time": "2024-12-04T10:59:47.5328638Z",
       "session_id": "36",
       "session_start_time": null,
       "spark_jobs": null,
       "spark_pool": "prdneudwcesyns",
       "state": "finished",
       "statement_id": 38,
       "statement_ids": [
        38
       ]
      },
      "text/plain": [
       "StatementMeta(prdneudwcesyns, 36, 38, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "34646258"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Aggregations without optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2024-12-04T11:01:09.090866Z",
       "execution_start_time": "2024-12-04T11:00:40.1444086Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "ca6586db-65bb-46de-8acd-c1a39a842ba0",
       "queued_time": "2024-12-04T10:59:47.6157611Z",
       "session_id": "36",
       "session_start_time": null,
       "spark_jobs": null,
       "spark_pool": "prdneudwcesyns",
       "state": "finished",
       "statement_id": 39,
       "statement_ids": [
        39
       ]
      },
      "text/plain": [
       "StatementMeta(prdneudwcesyns, 36, 39, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+-----+--------------------+\n",
      "|Ticker|Year|Month|             MaxHigh|\n",
      "+------+----+-----+--------------------+\n",
      "|   CVX|1962|    1|0.046914567869972224|\n",
      "|    BA|1962|    2|  0.2123341160576403|\n",
      "|   MMM|1962|    3|  0.5588248109436902|\n",
      "|    GT|1962|    7|  1.5118772841961448|\n",
      "|   CAT|1962|   10| 0.11590789252862449|\n",
      "|   DIS|1963|    5| 0.06025179644915724|\n",
      "|    PG|1963|    8|  0.2472103907752564|\n",
      "|    KR|1963|   12|0.029898689640230982|\n",
      "|   MMM|1963|   12|  0.5349178824265026|\n",
      "|    KR|1964|   10|0.035673110712459544|\n",
      "|    PG|1964|   12|  0.2616721498644771|\n",
      "|    BA|1964|   12|  0.3089230842982749|\n",
      "|    KR|1965|    3| 0.04350065218647663|\n",
      "|    IP|1965|    7|  0.8789791717211558|\n",
      "|   MCD|1967|   12|0.011546064415855302|\n",
      "|   HON|1968|    4|  1.1158002614974976|\n",
      "|   CNP|1969|    9| 0.31558356931370435|\n",
      "|   DTE|1970|    2| 0.41256187978109926|\n",
      "|   XOM|1970|    5|  0.1612526778363936|\n",
      "|   MCD|1970|    6| 0.00438635054789685|\n",
      "+------+----+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Time taken to run the query: 28.124725103378296 seconds\n"
     ]
    }
   ],
   "source": [
    "df2 = df.select(\"Date\", \"Ticker\", \"High\")\\\n",
    "        .withColumn(\"Year\", year(col(\"Date\")))\\\n",
    "        .withColumn(\"Month\", month(col(\"Date\")))\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "df = df.select(\"Date\", \"Ticker\", \"High\").withColumn(\"Year\", year(col(\"Date\"))).withColumn(\"Month\", month(col(\"Date\")))\n",
    "\n",
    "# Group by Ticker, Year, and Month, and calculate max High\n",
    "max_high_df = df2.groupBy(\"Ticker\", \"Year\", \"Month\").agg(max(\"High\").alias(\"MaxHigh\"))\n",
    "\n",
    "# Show the result\n",
    "max_high_df.show()\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate the elapsed time\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Time taken to run the query: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## optimization implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {},
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2024-12-04T11:23:55.3906216Z",
       "execution_start_time": "2024-12-04T11:22:56.7722211Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "b24c1192-0478-4835-b214-5e473302fd6e",
       "queued_time": "2024-12-04T11:22:56.6716398Z",
       "session_id": "36",
       "session_start_time": null,
       "spark_jobs": null,
       "spark_pool": "prdneudwcesyns",
       "state": "finished",
       "statement_id": 47,
       "statement_ids": [
        47
       ]
      },
      "text/plain": [
       "StatementMeta(prdneudwcesyns, 36, 47, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = spark.read.load('abfss://filesystem@flumenbaron.dfs.core.windows.net/synapse/workspaces/warehouse/all_stock_data.csv', format='csv', header = True)\n",
    "\n",
    "# Extract year from Date and add it as a new column\n",
    "df = df.withColumn(\"Year\", year(col(\"Date\")))\n",
    "\n",
    "# Repartition the DataFrame by the Year column\n",
    "partitioned_df = df.repartition(\"Year\")\n",
    "\n",
    "# Save the partitioned DataFrame to a Parquet file\n",
    "partitioned_df.write.partitionBy(\"Year\").parquet(\"abfss://filesystem@flumenbaron.dfs.core.windows.net/synapse/workspaces/warehouse/all_stock_data.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2024-12-04T11:24:47.5082903Z",
       "execution_start_time": "2024-12-04T11:24:46.4571168Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "6911e64e-3b3d-49eb-b795-ee0c1a773973",
       "queued_time": "2024-12-04T11:24:46.3407754Z",
       "session_id": "36",
       "session_start_time": null,
       "spark_jobs": null,
       "spark_pool": "prdneudwcesyns",
       "state": "finished",
       "statement_id": 50,
       "statement_ids": [
        50
       ]
      },
      "text/plain": [
       "StatementMeta(prdneudwcesyns, 36, 50, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the path to the CSV file\n",
    "df3 = spark.read.load(\n",
    "    'abfss://filesystem@flumenbaron.dfs.core.windows.net/synapse/workspaces/warehouse/all_stock_data.csv',\n",
    "    format='parquet',\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "df3 = df3.select(\"Date\", \"Ticker\", \"High\")\\\n",
    "        .withColumn(\"Year\", year(col(\"Date\")))\\\n",
    "        .withColumn(\"Month\", month(col(\"Date\")))\n",
    " \n",
    "\n",
    "# Extract year and month from Date\n",
    "df3 = df3.withColumn(\"Year\", year(col(\"Date\"))).withColumn(\"Month\", month(col(\"Date\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2024-12-04T11:25:12.6859287Z",
       "execution_start_time": "2024-12-04T11:24:56.0480337Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "1c112b82-66e3-4271-8552-445c4541394a",
       "queued_time": "2024-12-04T11:24:55.9161489Z",
       "session_id": "36",
       "session_start_time": null,
       "spark_jobs": null,
       "spark_pool": "prdneudwcesyns",
       "state": "finished",
       "statement_id": 51,
       "statement_ids": [
        51
       ]
      },
      "text/plain": [
       "StatementMeta(prdneudwcesyns, 36, 51, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+-----+------------------+\n",
      "|Ticker|Year|Month|           MaxHigh|\n",
      "+------+----+-----+------------------+\n",
      "|     A|1999|   11|30.757480725338883|\n",
      "|     A|2000|    8|39.369579881907896|\n",
      "|     A|2002|    7|15.052704100851255|\n",
      "|     A|2002|    9| 9.891603317935147|\n",
      "|     A|2003|    2| 8.427548509398077|\n",
      "|     A|2003|    4| 9.983874566925232|\n",
      "|     A|2004|    1|23.867802475774685|\n",
      "|     A|2004|   11|15.932374147296567|\n",
      "|     A|2005|    5| 14.85585587663277|\n",
      "|     A|2005|    6|15.409495064471285|\n",
      "|     A|2005|   12| 22.20690469593362|\n",
      "|     A|2006|   12|23.301044006029702|\n",
      "|     A|2007|    4| 23.37285799781407|\n",
      "|     A|2007|    6|25.749318295548324|\n",
      "|     A|2007|   12|24.965866627893188|\n",
      "|     A|2008|   11|16.367530080331825|\n",
      "|     A|2008|   12| 9.884502004315255|\n",
      "|     A|2009|    3| 9.760454276178807|\n",
      "|     A|2010|    3|22.687340376925135|\n",
      "|     A|2010|    4| 24.43704347695698|\n",
      "+------+----+-----+------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Time taken to run the query: 15.86033296585083 seconds\n"
     ]
    }
   ],
   "source": [
    "# Extract year and month from Date\n",
    "df_opt = df3.withColumn(\"Year\", year(col(\"Date\"))).withColumn(\"Month\", month(col(\"Date\")))\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Group by Ticker, Year, and Month, and calculate max High\n",
    "max_high_df = df_opt.groupBy(\"Ticker\", \"Year\", \"Month\").agg(max(\"High\").alias(\"MaxHigh\"))\n",
    "\n",
    "# Trigger an action to execute the transformations\n",
    "max_high_df.show()\n",
    "\n",
    "# Stop the timer\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate the elapsed time\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Time taken to run the query: {elapsed_time} seconds\")"
   ]
  }
 ],
 "metadata": {
  "description": null,
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Synapse PySpark",
   "language": "Python",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "save_output": true,
  "synapse_widget": {
   "state": {},
   "version": "0.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
